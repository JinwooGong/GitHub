{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF 생성 후 심층 신경망을 이용한 이메일 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = newsgroups_train.data\n",
    "x_test = newsgroups_test.data\n",
    "\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all 20 categories\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "\n",
      "Sample Email:\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sample Target Category:\n",
      "7\n",
      "rec.autos\n"
     ]
    }
   ],
   "source": [
    "print('List of all 20 categories')\n",
    "print(newsgroups_train.target_names)\n",
    "print('\\n')\n",
    "print('Sample Email:')\n",
    "print(x_train[0])\n",
    "print('Sample Target Category:')\n",
    "print(y_train[0])\n",
    "print(newsgroups_train.target_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리에 사용\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    # 단어를 분할해 각 문자에 표준 문장부호가 포함되어 있는지 확인\n",
    "    # 표준 문장부호가 있다면 빈칸으로 변경\n",
    "    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
    "    \n",
    "    # 문장을 공백에 따라 단어로 토큰화하고 추가 단계를 적용하기 위한 리스트로 묶는다.\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)]\n",
    "    # 모든 문자를 소문자로 변환해 말뭉치에서 중복을 제거한다.\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # 불용어 제거\n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    # 3글자 이상의 단어들만을 추출\n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    # 단어에서 추가로 접미사가 나오는 단어에 PorterStemmer를 사용해 스테밍을 적용한다.\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    # 품사 태깅\n",
    "    tagged_corpus = pos_tag(tokens)\n",
    "    \n",
    "    #pos_tag 함수는 명사에 대한 네가지 형태와 동사의 여섯가지 형태로 품사를 반환한다.\n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    Verb_tags = ['VB','VBD','VBG',' VBN','VBP','VBZ']\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def prat_lemmatize(token,tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        \n",
    "    pre_proc_text = \" \".join([prat_lemmatize(token,tag) for token, tag in tagged_corpus])\n",
    "    \n",
    "    return pre_proc_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed = []\n",
    "for i in x_train:\n",
    "    x_train_preprocessed.append(preprocessing(i))\n",
    "    \n",
    "x_test_preproceesed = []\n",
    "for i in x_test:\n",
    "    x_test_preproceesed.append(preprocessing(i))\n",
    "    \n",
    "# TFIDF 벡터라이저(vectorizer) 구축\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2),stop_words='english', max_features= 10000,strip_accents='unicode',norm='l2')\n",
    "x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()\n",
    "x_test_2 = vectorizer.transform(x_test_preproceesed).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391, 566)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train_preprocessed[0]), len(x_train_preprocessed[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lerxst wam umd edu thing subject car nntp post host rac3 wam umd edu organ univers maryland colleg park line wonder anyon could enlighten car saw day door sport car look late 60 earli 70 call bricklin door realli small addit front bumper separ rest bodi know anyon tellm model name engin spec year product car make histori whatev info funki look car plea mail thank bring neighborhood lerxst'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_preprocessed[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_2[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 모듈\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adadelta,Adam,RMSprop\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 정의\n",
    "np.random.seed(1337)\n",
    "nb_classes = 20\n",
    "batch_size = 64\n",
    "nb_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 1000)              10001000  \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 50)                25050     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 10,527,570\n",
      "Trainable params: 10,527,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 케라스에서의 딥 레이어(심층) 모델 구축\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1000,input_shape = (10000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer = 'adam')\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "177/177 [==============================] - 10s 55ms/step - loss: 1.9971\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 10s 54ms/step - loss: 0.6385\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 10s 55ms/step - loss: 0.3165\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 9s 54ms/step - loss: 0.1770\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 10s 54ms/step - loss: 0.1169\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 10s 54ms/step - loss: 0.0872\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 10s 54ms/step - loss: 0.0688\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 10s 54ms/step - loss: 0.0532\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 10s 54ms/step - loss: 0.0488\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 10s 54ms/step - loss: 0.0465\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 10s 55ms/step - loss: 0.0400\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 10s 55ms/step - loss: 0.0386\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 10s 55ms/step - loss: 0.0303\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 10s 55ms/step - loss: 0.0324\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 10s 55ms/step - loss: 0.0286\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 10s 56ms/step - loss: 0.0282\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 10s 56ms/step - loss: 0.0267\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 10s 56ms/step - loss: 0.0254\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 10s 56ms/step - loss: 0.0233\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 10s 56ms/step - loss: 0.0230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fecd73ccaf0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "model.fit(x_train_2, Y_train, batch_size=batch_size, epochs = nb_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-e1412e0776d3>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "# 모델 예측\n",
    "y_train_predclass = model.predict_classes(x_train_2,batch_size=batch_size)\n",
    "y_test_predclass = model.predict_classes(x_test_2,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Deep Neural Network - Train accuracy:\n",
      "\n",
      "Deep Neural Network - Test accuracy:\n",
      "\n",
      "Deep Neural Network - Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       480\n",
      "           1       1.00      1.00      1.00       584\n",
      "           2       1.00      1.00      1.00       591\n",
      "           3       1.00      1.00      1.00       590\n",
      "           4       1.00      1.00      1.00       578\n",
      "           5       1.00      1.00      1.00       593\n",
      "           6       1.00      1.00      1.00       585\n",
      "           7       1.00      1.00      1.00       594\n",
      "           8       1.00      1.00      1.00       598\n",
      "           9       1.00      1.00      1.00       597\n",
      "          10       1.00      1.00      1.00       600\n",
      "          11       1.00      1.00      1.00       595\n",
      "          12       1.00      1.00      1.00       591\n",
      "          13       1.00      1.00      1.00       594\n",
      "          14       1.00      1.00      1.00       593\n",
      "          15       1.00      1.00      1.00       599\n",
      "          16       1.00      1.00      1.00       546\n",
      "          17       1.00      1.00      1.00       564\n",
      "          18       1.00      1.00      1.00       465\n",
      "          19       1.00      1.00      1.00       377\n",
      "\n",
      "    accuracy                           1.00     11314\n",
      "   macro avg       1.00      1.00      1.00     11314\n",
      "weighted avg       1.00      1.00      1.00     11314\n",
      "\n",
      "\n",
      "Deep Neural Network - Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.73      0.76       319\n",
      "           1       0.74      0.71      0.72       389\n",
      "           2       0.67      0.67      0.67       394\n",
      "           3       0.62      0.69      0.66       392\n",
      "           4       0.71      0.81      0.76       385\n",
      "           5       0.86      0.72      0.79       395\n",
      "           6       0.76      0.86      0.80       390\n",
      "           7       0.85      0.88      0.87       396\n",
      "           8       0.90      0.92      0.91       398\n",
      "           9       0.93      0.90      0.91       397\n",
      "          10       0.94      0.97      0.95       399\n",
      "          11       0.93      0.92      0.92       396\n",
      "          12       0.71      0.73      0.72       393\n",
      "          13       0.93      0.78      0.85       396\n",
      "          14       0.89      0.89      0.89       394\n",
      "          15       0.79      0.90      0.84       398\n",
      "          16       0.79      0.82      0.80       364\n",
      "          17       0.95      0.88      0.91       376\n",
      "          18       0.78      0.62      0.69       310\n",
      "          19       0.59      0.64      0.61       251\n",
      "\n",
      "    accuracy                           0.81      7532\n",
      "   macro avg       0.81      0.80      0.80      7532\n",
      "weighted avg       0.81      0.81      0.81      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "print (\"\\n\\nDeep Neural Network - Train accuracy:\"),(round(accuracy_score(y_train,y_train_predclass),3))\n",
    "print (\"\\nDeep Neural Network - Test accuracy:\"),(round(accuracy_score(y_test,y_test_predclass),3))\n",
    "print (\"\\nDeep Neural Network - Train Classification Report\")\n",
    "print (classification_report(y_train,y_train_predclass))\n",
    "print (\"\\nDeep Neural Network - Test Classification Report\")\n",
    "print (classification_report(y_test,y_test_predclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
